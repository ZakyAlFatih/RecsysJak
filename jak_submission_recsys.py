# -*- coding: utf-8 -*-
"""Jak_Submission_Recsys.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12NY9z_o0ATqKdnJwz2E2q0Mp3Gnjt9VT

#  REKOMENDASI FILM

Dataset yang digunakan adalah Movie Dataset yang berasal dari  (https://www.kaggle.com/datasets/rounakbanik/the-movies-dataset)

**by zakyalfatih - zakyal2004@gmail.com**

# DATA LOADING

## Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
# Library bawaan Python
import os
import shutil
import zipfile
from ast import literal_eval

# Manipulasi dan analisis data
import pandas as pd
import numpy as np
from scipy import stats
from scipy.sparse import csr_matrix

# Visualisasi
import seaborn as sns
import matplotlib.pyplot as plt
import missingno as msno
from wordcloud import WordCloud
# %matplotlib inline

# Machine Learning dan Preprocessing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import shuffle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score

# TensorFlow dan Keras untuk Deep Learning
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Input, Embedding, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from keras.callbacks import EarlyStopping

"""##  Extrak Dataset

Selanjutnya, kita akan mengekstrak file zip dan mengambil dataset dalam format csv
"""

# Path file zip dan direktori tujuan ekstrak
zip_path = "/content/movie_recs.zip"
extract_path = "/content/movie_recs"

# Ekstrak zip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("‚úÖ File berhasil diekstrak ke:", extract_path)

# Lihat semua file CSV yang berhasil diekstrak
for filename in os.listdir(extract_path):
    print("üìÑ", filename)

"""## Reading Data"""

# Membaca file dataset
movies = pd.read_csv(f"{extract_path}/movies_metadata.csv", low_memory=False)
ratings = pd.read_csv(f"{extract_path}/ratings_small.csv", low_memory=False)
ratings_all = pd.read_csv(f"{extract_path}/ratings.csv", low_memory=False)
keywords = pd.read_csv(f"{extract_path}/keywords.csv", low_memory=False)
credits = pd.read_csv(f"{extract_path}/credits.csv", low_memory=False)
links = pd.read_csv(f"{extract_path}/links.csv", low_memory=False)
links_small = pd.read_csv(f"{extract_path}/links_small.csv", low_memory=False)

print("‚úÖ Semua file berhasil dibaca.")

print("="*100)
print("üìä Rangkuman Statistik Dataset The Movies Dataset")
print("="*100)

# Dataset Movies
print(f"üé¨ Jumlah film unik: {movies['id'].nunique()}")

print("-"*100)

# Dataset Rating Lengkap (ratings.csv)
print(f"‚≠ê Jumlah rating (lengkap): {len(ratings_all)}")
print(f"üë§ Jumlah pengguna unik (lengkap): {ratings_all['userId'].nunique()}")
print(f"üéûÔ∏è Jumlah film unik yang diberi rating (lengkap): {ratings_all['movieId'].nunique()}")

print("-"*100)

# Dataset Rating Kecil (ratings_small.csv)
print(f"‚≠ê Jumlah rating (kecil): {len(ratings)}")
print(f"üë§ Jumlah pengguna unik (kecil): {ratings['userId'].nunique()}")
print(f"üéûÔ∏è Jumlah film unik yang diberi rating (kecil): {ratings['movieId'].nunique()}")

print("-"*100)

# Dataset Keywords
print(f"üóùÔ∏è Jumlah film dengan kata kunci: {keywords['id'].nunique()}")

print("-"*100)

# Dataset Credits
print(f"üé≠ Jumlah film dengan informasi kru dan pemeran: {credits['id'].nunique()}")

print("-"*100)

# Dataset Links
print(f"üîó Jumlah data ID TMDB ‚Üî IMDB (lengkap): {links['movieId'].nunique()}")
print(f"üîó Jumlah data ID TMDB ‚Üî IMDB (kecil): {links_small['movieId'].nunique()}")

print("="*100)

"""Berdasarkan variabel-variabel yang tersedia dalam dataset di atas, kita hanya akan menggunakan dua variabel utama yang relevan untuk keperluan analisis dan pelatihan model pada proyek ini, yaitu dataset movies dan ratings.

# Exploratory Data Analysis (EDA)

## Analisis Univariat  
Pada tahap ini, kita akan melakukan eksplorasi terhadap masing-masing variabel dalam dataset secara terpisah. Visualisasi akan digunakan untuk menunjukkan distribusi genre dan rating film, serta untuk memahami karakteristik dan pola dari setiap fitur yang dianalisis.

### Deskripsi Variabel  
Pada tahap ini, kita akan menampilkan informasi terkait variabel `ratings` dan `movies` menggunakan fungsi `info`. Kita mulai dengan memeriksa variabel `ratings` terlebih dahulu.
"""

ratings.info()

"""Berdasarkan gambar di atas, variabel `ratings` memiliki 100.004 baris dan 4 kolom dengan penjelasan sebagai berikut:

| Variabel    | Deskripsi                                                                                         |
|-------------|-------------------------------------------------------------------------------------------------|
| userId      | ID unik yang mewakili pengguna yang memberikan rating, digunakan untuk mengidentifikasi pengguna secara anonim. |
| movieId     | ID unik untuk film yang dinilai oleh pengguna, berguna untuk menghubungkan dengan informasi film lebih detail.        |
| rating      | Nilai penilaian yang diberikan pengguna terhadap film, dengan skala 1 sampai 5, di mana angka lebih tinggi menunjukkan rating yang lebih positif. |
| timestamp   | Waktu saat rating diberikan, dalam format UNIX timestamp (jumlah detik sejak 1 Januari 1970).                       |

cek variabel movies
"""

movies.info()

"""Berdasarkan gambar di atas, variabel `movies` terdiri dari 100.004 baris dan 24 kolom, dengan penjelasan masing-masing sebagai berikut:

| Variabel             | Deskripsi                                                                                              |
|----------------------|------------------------------------------------------------------------------------------------------|
| adult                | Menunjukkan apakah film ditujukan untuk penonton dewasa (adult content), biasanya bernilai True atau False. |
| belongs_to_collection | Informasi mengenai koleksi atau seri film yang mencakup film ini (misalnya seri Harry Potter), biasanya berupa JSON atau string deskriptif. |
| budget               | Anggaran produksi film, biasanya dalam satuan mata uang seperti USD, berupa angka numerik.           |
| genres               | Daftar genre film seperti Action, Comedy, Drama, umumnya dalam format JSON atau daftar string.        |
| homepage             | URL situs resmi film tersebut.                                                                        |
| id                   | ID unik film yang biasanya merujuk ke database film seperti TMDb.                                     |
| imdb_id              | ID unik film di IMDb, contohnya "tt1234567".                                                         |
| original_language    | Bahasa asli film, menggunakan kode bahasa ISO 639-1 (misalnya "en" untuk bahasa Inggris).             |
| original_title       | Judul asli film dalam bahasa produksinya.                                                            |
| overview             | Ringkasan atau sinopsis film.                                                                         |
| popularity           | Skor popularitas film berdasarkan algoritma tertentu dari platform film.                              |
| poster_path          | Path atau tautan ke gambar poster film, biasanya digabungkan dengan URL dasar untuk mengaksesnya.     |
| production_companies | Informasi tentang perusahaan produksi film, biasanya berupa JSON yang mencantumkan nama dan ID perusahaan. |
| production_countries | Negara tempat film diproduksi, biasanya dalam format JSON dengan nama dan kode negara.                |
| release_date         | Tanggal rilis film dengan format YYYY-MM-DD.                                                         |
| revenue              | Pendapatan kotor film, biasanya dalam satuan mata uang seperti USD.                                  |
| runtime              | Durasi film dalam satuan menit.                                                                       |
| spoken_languages     | Bahasa yang digunakan dalam dialog film, biasanya dalam format JSON dengan nama dan kode bahasa.      |
| status               | Status rilis film, misalnya "Released" atau "In Production".                                          |
| tagline              | Slogan atau frasa singkat yang digunakan untuk promosi film.                                         |
| title                | Judul utama film yang digunakan untuk tujuan promosi.                                                |
| video                | Menandakan apakah ada video terkait film, biasanya bernilai True atau False.                          |
| vote_average         | Nilai rata-rata yang diberikan pengguna (misalnya IMDb atau TMDb), biasanya pada skala 1 sampai 10.  |
| vote_count           | Jumlah suara atau ulasan yang diberikan untuk film tersebut.                                         |

diatas merupakan informasi variabel movies dan rating dataset kita.

###  Infromasi Tipe Data

Pada tahap ini, kita akan memeriksa informasi yang terkandung dalam dataset *ratings* dan *movies* untuk memahami struktur serta isi datanya.
"""

tipe_data_movies = movies.dtypes.value_counts()
tipe_data_movies

tipe_data_ratings = ratings.dtypes.value_counts()
tipe_data_ratings

"""Dari informasi dataset, dapat diketahui bahwa pada dataset **movies** terdapat 20 variabel dengan tipe data *object* dan 4 variabel dengan tipe data *float64*. Sementara itu, pada dataset **ratings** terdapat 1 variabel bertipe *float64* dan 3 variabel bertipe *int64*.

### Menghitung Total Data

Pertama, kita akan melihat jumlah total data pada dataset `ratings` dan `movies`.
"""

rows_movie = movies.shape[0]
cols_movie = movies.shape[1]
rows_rating = ratings.shape[0]
cols_rating = ratings.shape[1]

print(f'Jumlah data Movie sebanyak {rows_movie}, dan memiliki {cols_movie} kolom')
print(f'Jumlah data Rating sebanyak {rows_rating}, dan memiliki {cols_rating} kolom')

"""### Menghitung Total Data Unik

"""

print(f"Jumlah film unik pada dataset movies : {movies['id'].nunique()}")
print(f"Jumlah film unik pada dataset ratings: {ratings['movieId'].nunique()}")
print(f"Jumlah user unik pada dataset ratings: {ratings['userId'].nunique()}")

"""Dari hasil di atas, terdapat 45.436 film pada dataset *movies*, 9.066 film yang mendapatkan rating pada dataset *ratings*, serta 671 pengguna unik yang memberikan rating pada dataset *ratings*.

### Melihat Statistik Deskriptif pada Variabel Dataset
"""

movies.describe()

ratings.describe()

"""Berdasarkan tampilan statistik deskriptif dari dataset *movies* dan *ratings*, tidak terlihat adanya penyebaran nilai yang mencolok atau indikasi keberadaan *outlier* yang signifikan.

### Sebaran Ratings

Pada tahap ini, kita akan menganalisis distribusi ratings dengan tujuan:  
1. Mengetahui nilai rating yang paling sering diberikan oleh pengguna.  
2. Menilai kecenderungan data rating, apakah lebih banyak rating tinggi, rendah, atau tersebar merata.  
3. Memahami pola preferensi pengguna terhadap film berdasarkan rating yang diberikan.
"""

# Mengelompokkan data berdasarkan kolom 'rating' dan menghitung jumlah baris untuk tiap nilai rating
rating_counts = ratings.groupby('rating').size().to_frame('count')

# Menghitung persentase setiap nilai rating terhadap total keseluruhan
rating_counts['percentage'] = round((rating_counts['count'] / rating_counts['count'].sum()) * 100, 1)

# Membuat diagram batang untuk memvisualisasikan persentase rating
ax = rating_counts['percentage'].plot.bar(color='#FF6F61', figsize=(8, 5))

# Menambahkan label persentase di atas tiap batang
for i, val in enumerate(rating_counts['percentage']):
    plt.text(i, val + 0.5, f'{val}%', ha='center', fontsize=10)

# Memberi label pada sumbu dan judul grafik
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Persentase (%)', fontsize=12)
plt.title('Distribusi Persentase Rating', fontsize=14)
plt.show()

"""Berdasarkan diagram distribusi rating di atas, terlihat bahwa nilai rating yang paling sering diberikan pengguna adalah 4.0 dengan persentase 28,7%, diikuti oleh rating 3.0 sebesar 20,1%, dan rating 5.0 sebesar 15,1%. Nilai rating lainnya memiliki persentase di bawah 12,0%.

### Sebaran Genre Film

Pada tahap ini, kita akan membersihkan, memproses, dan menormalkan data dalam kolom `genres` pada DataFrame `df_movies`. Beberapa fungsi yang digunakan adalah:

- `fillna('[]')`: Mengisi nilai `null` atau `NaN` pada kolom `genres` dengan string kosong berbentuk list ('[]').
- `apply(literal_eval)`: Menggunakan fungsi `literal_eval` dari modul `ast` untuk mengubah string yang menyerupai literal Python menjadi tipe data `list`.
- `apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])`: Fungsi lambda ini memproses setiap elemen dalam kolom `genres` dengan cara berikut:
  - Jika nilai merupakan sebuah list (`isinstance(x, list)`), maka diambil nilai dari kunci `name` untuk setiap item di dalam list tersebut.
  - Jika bukan list, maka mengembalikan list kosong (`[]`).

Langkah awal, kita buat sebuah variabel DataFrame baru untuk keperluan analisis dan visualisasi data. Selanjutnya, kita konversi fitur (variabel) `genres` ke dalam format list agar dapat dianalisis dengan lebih mudah.
"""

df_analisis_movies = movies.copy()
df_analisis_movies['genres'] = df_analisis_movies['genres'].fillna('[]') \
    .apply(literal_eval) \
    .apply(lambda x: [genre['name'] for genre in x] if isinstance(x, list) else [])

"""Selanjutnya, kita memecah setiap elemen dalam daftar genre menjadi baris terpisah menggunakan fungsi explode(). Kemudian, kita hitung frekuensi kemunculan setiap genre dengan value_counts(). Terakhir, kita buat diagram batang untuk memvisualisasikan distribusi genre menggunakan plot(kind='bar')."""

all_genres = df_analisis_movies['genres'].explode()
genre_counts = all_genres.value_counts()

print("Distribusi Genre:")
print(genre_counts)

colors = plt.cm.viridis(np.linspace(0, 1, len(genre_counts)))

plt.figure(figsize=(18, 6))
genre_counts.plot(kind='bar', color=colors)
plt.title('Distribusi dari Genre', fontsize=14)
plt.xlabel('Genre', fontsize=12)
plt.ylabel('Frekuensi', fontsize=12)
plt.xticks(rotation=45)
plt.show()

"""Berdasarkan grafik tersebut, dapat diamati bahwa genre Drama dan Comedy merupakan genre yang paling banyak muncul dalam dataset, dengan jumlah masing-masing sebanyak 20.265 dan 13.182 film. Sementara itu, genre-genre lainnya memiliki jumlah di bawah 10.000. Selain itu, terdapat 12 genre yang hanya muncul sebanyak 1 kali saja.

### Analisis Film dengan Skor Tertinggi di Seluruh Periode

Untuk membuat daftar film dengan skor tertinggi, digunakan metode Weighted Score. Metode ini menghitung skor berbobot dengan menggabungkan beberapa nilai berdasarkan tingkat kepentingannya.

Dalam konteks film, skor berbobot dihitung menggunakan beberapa parameter:

Rata-rata penilaian film (vote_average)

Jumlah suara yang diberikan pada film tersebut (vote_count)

Ambang batas jumlah suara minimum yang harus dipenuhi agar film dapat dipertimbangkan dalam daftar (threshold)

Keterangan variabel:

v = jumlah suara untuk film tertentu (vote_count)

m = jumlah suara minimum agar film bisa masuk daftar (threshold)

R = rata-rata skor film (vote_average)

C = rata-rata skor seluruh film dalam dataset (rata-rata global)
"""

# Hitung rata-rata skor global (C)
C = df_analisis_movies['vote_average'].mean()

# Ambil kolom rata-rata skor (R) dan jumlah suara (v)
R = df_analisis_movies['vote_average']
v = df_analisis_movies['vote_count']

# Tentukan threshold suara minimum (m) sebagai persentil ke-75
m = df_analisis_movies['vote_count'].quantile(0.75)

# Hitung Weighted Score menggunakan rumus berbobot
df_analisis_movies['weighted_score'] = (v / (v + m)) * R + (m / (v + m)) * C

# Urutkan film berdasarkan weighted_score dari tertinggi ke terendah
top_movies = df_analisis_movies.sort_values(by='weighted_score', ascending=False)

# Tampilkan 5 film teratas berdasarkan weighted score
print("Daftar 5 Film dengan Skor Tertinggi:")
top_movies[['id', 'genres', 'title', 'vote_average', 'vote_count', 'weighted_score']].head(5)

"""Pada tabel di atas tercantum 5 film teratas dengan nilai skor tertinggi yang diberikan oleh para pengguna.

### Analisis Film dengan Rating Tertinggi

Selanjutnya, kita akan menggabungkan dataset df_movies dan ratings menggunakan fungsi pd.merge dari pandas. Setelah penggabungan, kita akan mencari 10 film dengan rating tertinggi.
"""

# Buat salinan dataset movies untuk analisis agar data asli tidak berubah
movies_copy = movies.copy()

# Filter hanya data dengan 'id' yang berupa angka valid
movies_copy = movies_copy[movies_copy['id'].apply(lambda x: str(x).isnumeric())]

# Ubah tipe data kolom 'id' menjadi integer 64-bit
movies_copy['id'] = movies_copy['id'].astype('int64')

# Ganti nama kolom 'id' menjadi 'movieId' agar sesuai dengan dataset ratings
movies_copy.rename(columns={'id': 'movieId'}, inplace=True)

# Salin hasil pembersihan ke variabel baru
df_movies = movies_copy.copy()

# Gabungkan dataset ratings dengan df_movies berdasarkan 'movieId'
rating_movies = pd.merge(ratings, df_movies, on='movieId', how='inner')

# Hapus kolom yang tidak diperlukan
rating_movies.drop(columns=['timestamp', 'genres'], inplace=True)

# Hitung rata-rata rating dan total jumlah rating per judul film
df_analisis_rating = rating_movies.groupby('title')['rating'].agg(['mean', 'count']).reset_index()

# Ganti nama kolom agar lebih deskriptif
df_analisis_rating.rename(columns={'mean': 'mean ratings', 'count': 'total ratings'}, inplace=True)

# Tampilkan 10 film dengan jumlah rating terbanyak
df_analisis_rating.sort_values('total ratings', ascending=False).head(10)

"""Dari 10 film dengan rating tertinggi, Terminator 3: Rise of the Machines menempati posisi teratas dengan rata-rata rating sebesar 4.256 dan jumlah total rating mencapai 324.

### Perbandingan Rata-Rata Peringkat dan Total Jumlah Peringkat

Pada tahap ini, kita akan membandingkan rata-rata peringkat dan total peringkat menggunakan visualisasi jointplot. Tujuannya adalah untuk melihat sebaran dan hubungan antara kedua variabel tersebut secara lebih mendetail.
"""

sns.jointplot(
    data=df_analisis_rating,
    x='mean ratings',
    y='total ratings',
    kind='scatter',      # jenis plot (scatter plot)
    height=8,            # ukuran plot
    marginal_ticks=True, # menampilkan ticks di histogram marginal
    alpha=0.6            # transparansi titik
)

plt.suptitle('Sebaran Mean Ratings vs Total Ratings', y=1.02, fontsize=14)
plt.show()

"""Berdasarkan grafik sebaran di atas, terdapat 5 film yang memiliki total rating lebih dari 250. Sementara itu, mayoritas pengguna memberikan rating film dalam rentang nilai antara 2 hingga 4.5.

# DATA PREPARATION

## Data Clean
Tahap ini bertujuan untuk mempersiapkan data mentah agar siap digunakan secara optimal dalam model machine learning. Setelah data dikumpulkan, terdapat beberapa langkah penting yang perlu dilakukan dalam proses ini, yaitu:

### Ekstraksi Fitur untuk Keperluan Model

Pada tahap ini, kita memilih beberapa fitur atau kolom dari variabel movies yang relevan untuk analisis dan pengolahan data, yaitu: ['id', 'genres', 'title', 'vote_average', 'vote_count']. Hasil seleksi fitur tersebut kemudian disimpan dalam variabel baru bernama df_movies.
"""

# Menentukan fitur yang relevan untuk analisis
selected_columns = ['id', 'genres', 'title', 'vote_average', 'vote_count']

# Membuat DataFrame baru hanya dengan kolom yang dipilih
df_movies = movies[selected_columns].copy()

# Menampilkan 5 baris pertama dari DataFrame hasil seleksi
df_movies.head()

"""### Penyesuaian Tipe Data untuk Primary Key dan Foreign Key

Berdasarkan informasi sebelumnya, atribut id pada dataset movies yang berfungsi sebagai primary key memiliki tipe data object, sedangkan pada dataset ratings, atribut yang berelasi yaitu movieId bertipe int64. Oleh karena itu, perlu dilakukan penyesuaian dengan menyamakan nama kolom menjadi movieId dan mengubah tipe data menjadi int64 agar keduanya dapat digabungkan dengan benar.
"""

# Salin dataset df_movies untuk menghindari perubahan data asli
df_movies_cleaned = df_movies.copy()

# Filter baris dengan nilai 'id' yang valid (hanya angka)
df_movies_cleaned = df_movies_cleaned[df_movies_cleaned['id'].apply(lambda x: str(x).isnumeric())]

# Ubah tipe data kolom 'id' menjadi int64
df_movies_cleaned['id'] = df_movies_cleaned['id'].astype('int64')

# Ubah nama kolom 'id' menjadi 'movieId' untuk konsistensi dengan dataset lain
df_movies_cleaned.rename(columns={'id': 'movieId'}, inplace=True)

# Simpan hasil akhir ke df_movies
df_movies = df_movies_cleaned.copy()

"""### Menangani Missing Value

Pada tahap ini, kita akan melakukan pengecekan terhadap nilai kosong (missing values) pada variabel dataset df_movies dan ratings.
"""

# Mengecek jumlah nilai kosong pada setiap kolom di dataset df_movies
df_movies.isnull().sum()

"""Dari hasil pengecekan tersebut, ditemukan bahwa kolom title, vote_average, dan vote_count masing-masing memiliki 3 nilai kosong (null).


"""

# Menampilkan baris pada df_movies yang mengandung setidaknya satu nilai null
df_movies[df_movies.isnull().any(axis=1)]

"""Dari hasil tersebut, terlihat bahwa terdapat beberapa baris dengan nilai kosong pada variabel dataset df_movies."""

df_movies = df_movies.dropna(subset=['title', 'vote_average', 'vote_count'])
df_movies.isnull().sum()

ratings.isnull().sum()

"""### Memeriksa Duplikasi pada Dataset

periksa variabel dataset df_movies
"""

# Menghitung jumlah baris duplikat dalam DataFrame df_movies
duplicate_count = df_movies.duplicated().sum()
print(f"Jumlah data duplikat: {duplicate_count}")

"""Menampilkan baris-baris yang duplikat berdasarkan semua kolom

"""

# Menampilkan movies duplikat
duplicate_rows = df_movies[df_movies.duplicated(keep=False)]
print(duplicate_rows)

"""Dari hasil tersebut, dapat dilihat bahwa beberapa data memang mengalami duplikasi."""

df_movies.drop_duplicates(inplace = True)

"""Periksa dataset ratings yang duplikat"""

ratings.duplicated().sum()

"""Karena hasil yang ditampilkan adalah 0, dapat disimpulkan bahwa tidak terdapat data duplikat pada dataset ratings.

## Data Preprocessing
Tahap ini bertujuan untuk mempersiapkan data mentah agar siap digunakan secara efektif dalam model machine learning. Langkah-langkah yang dilakukan dalam proyek ini meliputi:

- Mengurutkan data pengguna dan film berdasarkan ID masing-masing

- Mengubah fitur genres pada data film menjadi format list

- Menggabungkan dataset ratings dan movies menjadi satu dataset terpadu

- Menghapus fitur atau kolom yang tidak diperlukan untuk analisis

- Memilih subset dataset yang sesuai dengan kebutuhan analisis dan pemodelan

# Mengurutkan dataset ratings berdasarkan userId secara ascending
ratings_sorted = ratings.sort_values(by='userId').reset_index(drop=True)
"""

ratings   = ratings.sort_values('userId')
df_movies = df_movies.sort_values('movieId')

ratings.head()

df_movies.head()

"""### Mengubah fitur genres movie ke bentuk list"""

# Mengisi nilai kosong pada kolom 'genres' dengan string kosong '[]'
# Mengonversi string representasi list menjadi list Python asli menggunakan literal_eval
# Ekstrak nama genre dari setiap elemen (asumsi tiap elemen adalah dict dengan key 'name')
df_movies['genres'] = df_movies['genres'].fillna('[]').apply(literal_eval).apply(
    lambda x: [genre['name'] for genre in x] if isinstance(x, list) else []
)

# Menampilkan kolom genres hasil konversi
df_movies['genres']

"""### Penggabungan Data

mengabungkan dataset ratings dan dataset movies menggunakan fungsi `merge` dan mengahapus variabel `timestamp` yang dibutuhkan.
"""

# Menggabungkan dataset ratings dan df_movies berdasarkan kolom 'movieId'
df_movies_ratings = pd.merge(ratings, df_movies, on='movieId', how='inner')

# Menampilkan 5 baris pertama hasil penggabungan
df_movies_ratings.head()

"""### Menghapus fitur yang tidak diperlukan

menghapus fitur-firur yang tidak diperlukan yaitu `timestamp`, `vote_average` dan `vote_count`.
"""

df_movies_ratings.drop(['timestamp', 'vote_average', 'vote_count'],axis=1, inplace=True)

"""menggunakan fungsi `head` untuk melihat hasilnya."""

df_movies_ratings.head(10)

"""### Memilih Data yang Relevan untuk Analisis

Pada tahap ini, kita melakukan sampling acak sebanyak 20.000 data dari gabungan dataset movies dan ratings menggunakan fungsi shuffle dari sklearn.utils. Teknik ini bertujuan untuk mempermudah proses pengolahan data dan menghindari kemungkinan crash akibat ukuran dataset yang terlalu besar.
"""

from sklearn.utils import shuffle

# Mengacak dataset gabungan dan mengambil 20.000 baris pertama sebagai sample
df_sample_final = shuffle(df_movies_ratings).head(20000)

# Menampilkan hasil sample
df_sample_final

"""# Content Based Filtering (CBF)

Content-Based Filtering adalah metode sistem rekomendasi yang menyarankan item dengan melihat kesamaan karakteristik atau fitur dari item yang sebelumnya disukai atau diberi nilai oleh pengguna.

## A. Data Preparation

Teknik yang diterapkan adalah TF-IDF (Term Frequency-Inverse Document Frequency), yang digunakan untuk memberi bobot pada fitur dan menghitung tingkat kemiripan antar item. Pada proyek ini, fitur yang digunakan sebagai item adalah genre film (genres).

pertama periksa datset kita sesuai dengan kebutuhan yang kita inginkan dengan fungsi `info()`
"""

df_sample_final.info()

"""Dapat dilihat bahwa dataset kita berisi 20.000 baris dan 5 kolom, dengan komposisi tipe data berupa 1 kolom bertipe float64, 2 kolom bertipe int64, serta 2 kolom bertipe object.

Selanjutnya, kita lanjut ke tahap persiapan dengan membuat variabel preparation yang berisi DataFrame df_sample_final, lalu mengurutkannya berdasarkan kolom movieId.
"""

preparation = df_sample_final
preparation.sort_values('movieId')

# Menghapus baris duplikat berdasarkan kolom 'movieId' pada DataFrame preparation
preparation = preparation.drop_duplicates(subset='movieId')

# Menampilkan DataFrame hasil penghapusan duplikat
preparation

"""Selanjutnya, kita akan mengubah data dalam bentuk series menjadi list menggunakan fungsi tolist() dari library numpy."""

# Mengonversi kolom 'movieId', 'title', dan 'genres' dari DataFrame preparation menjadi list
movie_id = preparation['movieId'].tolist()
movie_name = preparation['title'].tolist()
movie_genres = preparation['genres'].tolist()

# Menampilkan panjang masing-masing list untuk memastikan kesamaan jumlah data
print(len(movie_id))
print(len(movie_name))
print(len(movie_genres))

"""Selanjutnya, kita akan membuat dictionary yang memetakan pasangan key-value menggunakan data movie_id, movie_name, dan movie_genres yang sudah kita siapkan sebelumnya."""

# Membuat DataFrame baru dari list movie_id, movie_name, dan movie_genres
movies_new = pd.DataFrame({
    'id': movie_id,
    'movie_name': movie_name,
    'genres': movie_genres
})

# Menampilkan DataFrame yang telah dibuat
movies_new

"""Selanjutnya, kita akan menggunakan fungsi TfidfVectorizer untuk mengubah data genres. Namun sebelum itu, kita perlu mengonversi genres yang berbentuk list menjadi string agar bisa diproses."""

data = movies_new.copy()

# Mengubah kolom 'genres' dari list menjadi string dengan pemisah koma
data['genres'] = data['genres'].apply(lambda x: ', '.join(x) if isinstance(x, list) else '')

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi objek TfidfVectorizer
tf_cbf = TfidfVectorizer()

# Melatih vectorizer pada data kolom 'genres'
tf_cbf.fit(data['genres'])

# Mendapatkan daftar fitur (nama-nama genre) hasil ekstraksi
features = tf_cbf.get_feature_names_out()
features

"""Setelah memperoleh indeks seluruh genre film, data tersebut akan di-fit dan kemudian diubah menjadi bentuk matriks menggunakan transformasi."""

# Melatih TfidfVectorizer dan mengubah data 'genres' menjadi matriks TF-IDF
tfidf_matrix = tf_cbf.fit_transform(data['genres'])

# Menampilkan dimensi matriks TF-IDF (jumlah film, jumlah fitur genre)
print(tfidf_matrix.shape)

"""Setelah proses fit dan transformasi, diperoleh matriks berukuran (2266 x 22). Langkah berikutnya adalah mengubah vektor TF-IDF tersebut menjadi matriks padat menggunakan fungsi todense()."""

# Mengonversi matriks TF-IDF yang berbentuk sparse matrix menjadi matriks padat (dense matrix)
dense_tfidf_matrix = tfidf_matrix.todense()
dense_tfidf_matrix

"""Setelah matriks terbentuk, kita buat sebuah tabel yang memuat judul film beserta genre-nya berdasarkan representasi TF-IDF yang telah dibuat sebelumnya."""

# Membuat DataFrame yang menampilkan bobot TF-IDF untuk setiap genre pada tiap film
tfidf_df = pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf_cbf.get_feature_names_out(),
    index=data['movie_name']
)

tfidf_df.head()

"""## B. Modeling

Pada tahap ini, digunakan metode Cosine Similarity untuk mengukur tingkat kesamaan antara dua vektor dalam ruang multidimensi. Dalam proyek ini, metode tersebut diterapkan pada sistem rekomendasi Content-Based Filtering untuk memberikan rekomendasi film berdasarkan kesamaan genre yang telah disukai atau dinilai pengguna.

Nilai Cosine Similarity berada dalam rentang 0 hingga 1, di mana nilai 0 menunjukkan dua dokumen sama sekali tidak mirip, dan nilai 1 berarti kedua dokumen tersebut sangat mirip. Nilai ini berlaku dalam ruang berdimensi tinggi.

Untuk menerapkan content-based filtering dalam proyek ini, digunakan metode cosine similarity untuk mengukur kemiripan antar film berdasarkan fitur genre.
"""

# Menghitung cosine similarity pada matriks tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)

# Menampilkan matriks cosine similarity
cosine_sim

# Menghitung cosine similarity dan menyimpan dalam DataFrame dengan indeks dan kolom berupa judul film
cosine_sim_df = pd.DataFrame(cosine_sim, index=data["movie_name"], columns=data["movie_name"])
print('Shape:', cosine_sim_df.shape)

# Menampilkan sampel 10 film secara acak pada baris dan kolom dari matriks similarity
cosine_sim_df.sample(10, axis=1).sample(10, axis=0)

"""Berikutnya, kita akan membuat fungsi untuk merekomendasikan film berdasarkan kesamaan genre dengan menggunakan metode Top-N rekomendasi."""

def recommend_movies(movie_title, similarity_matrix=cosine_sim_df, movie_info=data[['movie_name', 'genres']], top_k=10):
    # Dapatkan indeks film berdasarkan skor kemiripan tertinggi
    indices = similarity_matrix[movie_title].to_numpy().argpartition(range(-1, -top_k - 1, -1))

    # Ambil nama-nama film dengan kemiripan tertinggi, kecuali film input
    similar_movies = similarity_matrix.columns[indices[-1:-(top_k+2):-1]].drop(movie_title, errors='ignore')

    # Gabungkan dengan data info film dan ambil top_k film
    return pd.DataFrame(similar_movies, columns=['movie_name']).merge(movie_info, on='movie_name').head(top_k)

"""## C. Uji Coba Sistem Rekomendasi

Di tahap ini, kita akan melakukan evaluasi terhadap model yang telah dibuat.
"""

data[data.movie_name.eq('Made in Hong Kong')]

# Mendapatkan rekomendasi film yang mirip dengan yang dipilih
recommended_movies = recommend_movies('Made in Hong Kong')
recommended_movies

"""Hasil yang diperoleh berupa daftar judul film yang memiliki kesamaan genre dengan film yang dipilih.

## D. Evaluation

Pada tahap evaluasi model, digunakan metrik precision untuk mengukur proporsi prediksi positif yang benar (True Positives) dari keseluruhan prediksi positif (True Positives ditambah False Positives).
"""

relevant_genres = {"Crime", "Drama", "Romance"}  # menggunakan set agar urutan tidak berpengaruh

def check_relevance(genres_str):
    genre_set = set(genres_str.split(', '))
    return relevant_genres.issubset(genre_set)

recommended_movies['is_relevant'] = recommended_movies['genres'].apply(check_relevance)

true_positives = recommended_movies['is_relevant'].sum()
total_recommended = len(recommended_movies)

precision = (true_positives / total_recommended) * 100
print(f"Precision: {precision:.2f}%")

"""Dari hasil tersebut, metrik precision mencapai 100.00% untuk 10 film yang direkomendasikan berdasarkan kesamaan genre.

# Collaborative Filtering (CF)

Model-Based Deep Learning Collaborative Filtering merupakan metode yang mengintegrasikan teknik collaborative filtering dengan deep learning guna meningkatkan akurasi dan performa sistem rekomendasi. Tahap ini melibatkan beberapa langkah persiapan penting sebelum pelatihan model, yang dapat dijelaskan sebagai berikut:

## A. Data Preparation

periksacek dataset kita dengan fungsi `info()`
"""

df_sample_final.info()

"""Berdasarkan hasil tersebut, dataset berisi 20.000 baris dan 5 kolom dengan komposisi tipe data terdiri dari 1 kolom bertipe float64, 2 kolom bertipe int64, serta 2 kolom bertipe object.

hapus kolom yang tidak dibutuhkan dalam pelatihan.
"""

# Menghapus kolom 'title'
df_sample_final = df_sample_final.drop(columns=['genres', 'title'])

""" urutkan berdasarkan kolom `userId`"""

df_sample_final = df_sample_final.sort_values('userId')

"""## Encoding untuk userId dan movieId

lakukan encoding pada `userId`
"""

# Mendapatkan daftar unik userID
user_ids = df_sample_final['userId'].unique().tolist()
print('Daftar userID:', user_ids)

# Membuat mapping userID ke angka (encoding)
user_to_user_encoded = {user_id: idx for idx, user_id in enumerate(user_ids)}
print('UserID setelah encoding:', user_to_user_encoded)

# Membalik mapping dari angka ke userID asli
user_encoded_to_user = {idx: user_id for idx, user_id in enumerate(user_ids)}
print('Mapping angka ke userID:', user_encoded_to_user)

"""selanjutnya,  lakukan hal yang sama pada `movieId`."""

# Mendapatkan daftar unik movieId
movie_ids = df_sample_final['movieId'].unique().tolist()
print('Daftar movieId:', movie_ids)

# Membuat mapping movieId ke angka (encoding)
movie_to_movie_encoded = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}
print('movieId setelah encoding:', movie_to_movie_encoded)

# Membalik mapping dari angka ke movieId asli
movie_encoded_to_movie = {idx: movie_id for idx, movie_id in enumerate(movie_ids)}
print('Mapping angka ke movieId:', movie_encoded_to_movie)

"""Setelah proses encoding selesai, kita lakukan pemetaan userId dan movieId ke dalam dataframe yang relevan."""

# Memetakan userId ke kolom 'user' pada dataframe
df_sample_final['user'] = df_sample_final['userId'].map(user_to_user_encoded)

# Memetakan movieId ke kolom 'movie' pada dataframe
df_sample_final['movie'] = df_sample_final['movieId'].map(movie_to_movie_encoded)

"""  ambil total_user, total movie dan nilai rating minimum dan maksimum untuk proses pembagian dataset sebelum melakukan pelatihan"""

# Menghitung total user
total_user = len(user_to_user_encoded)
print(total_user)

# Menghitung total movie
total_movies = len(movie_encoded_to_movie)
print(total_movies)

# Mengonversi kolom rating ke tipe float32
df_sample_final['rating'] = df_sample_final['rating'].astype(np.float32)

# Mendapatkan nilai rating minimum
min_rating = df_sample_final['rating'].min()

# Mendapatkan nilai rating maksimum
max_rating = df_sample_final['rating'].max()

print(f'Number of Users: {total_user}, Number of Movies: {total_movies}, Min Rating: {min_rating}, Max Rating: {max_rating}')

"""Hasil tersebut menunjukkan bahwa dataset berisi 671 pengguna, 2266 film, dengan rentang rating mulai dari 0.5 hingga 5.0.

### Pembagian Data untuk Training dan Validasi

Pada tahap ini, kita akan membagi dataset menjadi data pelatihan dan data validasi untuk keperluan training model. Sebelum itu, dataset perlu diacak terlebih dahulu agar data validasi yang dihasilkan representatif.
"""

# Mengacak dataset
df_sample_final = df_sample_final.sample(frac=1, random_state=42)
df_sample_final

"""Selanjutnya, kita membuat variabel x yang berisi gabungan informasi antara data pengguna (user) dan film (movie) menjadi satu representasi nilai. Kemudian, variabel y digunakan untuk menyimpan nilai rating sebagai label. Terakhir, dataset dibagi menjadi 80% data pelatihan dan 20% data validasi."""

# Membentuk fitur dari kolom 'user' dan 'movie'
features = df_sample_final[['user', 'movie']].values

# Menormalisasi rating ke rentang 0-1
labels = df_sample_final['rating'].apply(
    lambda rating: (rating - min_rating) / (max_rating - min_rating)
).values

# Menentukan batas indeks untuk pembagian data train dan validasi (80/20)
split_index = int(0.8 * len(df_sample_final))

# Membagi dataset menjadi data pelatihan dan validasi
x_train, x_val = features[:split_index], features[split_index:]
y_train, y_val = labels[:split_index], labels[split_index:]

# Menampilkan hasil akhir dari fitur dan label
print(features, labels)

"""## B. Modeling

model akan menghitung tingkat kecocokan antara pengguna dan film menggunakan teknik embedding. Langkah pertama adalah menerapkan embedding pada data pengguna dan film secara terpisah. Setelah itu, kita melakukan operasi dot product antara hasil embedding pengguna dan film untuk mendapatkan skor interaksi mereka. Sebagai tambahan, kita juga menyisipkan bias khusus untuk masing-masing pengguna dan film guna meningkatkan akurasi prediksi.

Skor akhir kecocokan akan dipetakan ke rentang [0, 1] menggunakan fungsi aktivasi sigmoid, sehingga dapat ditafsirkan sebagai probabilitas. Untuk mengimplementasikan model ini, kita membangun sebuah kelas bernama RecommenderNet yang diturunkan dari kelas Model milik Keras.
"""

class RecommenderNet(tf.keras.Model):
    def __init__(self, total_users, total_movies, embedding_size, **kwargs):
        super(RecommenderNet, self).__init__(**kwargs)

        self.total_users = total_users
        self.total_movies = total_movies
        self.embedding_size = embedding_size

        # Embedding untuk user
        self.user_embedding = layers.Embedding(
            input_dim=total_users,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.user_bias = layers.Embedding(
            input_dim=total_users,
            output_dim=1
        )

        # Embedding untuk movie
        self.movie_embedding = layers.Embedding(
            input_dim=total_movies,
            output_dim=embedding_size,
            embeddings_initializer='he_normal',
            embeddings_regularizer=keras.regularizers.l2(1e-6)
        )
        self.movie_bias = layers.Embedding(
            input_dim=total_movies,
            output_dim=1
        )

    def call(self, inputs):
        user_ids = inputs[:, 0]
        movie_ids = inputs[:, 1]

        # Ambil embedding dan bias masing-masing
        user_vector = self.user_embedding(user_ids)
        user_bias = self.user_bias(user_ids)
        movie_vector = self.movie_embedding(movie_ids)
        movie_bias = self.movie_bias(movie_ids)

        # Hitung dot product antar embedding user dan movie
        interaction = tf.reduce_sum(user_vector * movie_vector, axis=1, keepdims=True)

        # Tambahkan bias
        x = interaction + user_bias + movie_bias

        # Skor akhir (0 - 1) menggunakan sigmoid
        return tf.nn.sigmoid(x)

"""Selanjutnya, lakukan proses compile terhadap model."""

# Inisialisasi model RecommenderNet dengan ukuran embedding 50
model = RecommenderNet(total_users=total_user, total_movies=total_movies, embedding_size=50)

# Kompilasi model dengan konfigurasi loss, optimizer, dan metrik evaluasi
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss=tf.keras.losses.MeanSquaredError(),
    metrics=[
        tf.keras.metrics.MeanAbsoluteError(name="MAE"),
        tf.keras.metrics.RootMeanSquaredError(name="RMSE")
    ]
)

"""Model ini dikompilasi menggunakan Binary Crossentropy sebagai fungsi loss untuk mengukur perbedaan antara prediksi dan label aktual. Sebagai algoritma optimisasi, digunakan Adam (Adaptive Moment Estimation) karena kemampuannya menyesuaikan laju pembelajaran secara adaptif. Untuk evaluasi performa, model menggunakan metrik Root Mean Squared Error (RMSE) guna mengukur rata-rata kesalahan prediksi terhadap data sebenarnya.

Langkah selanjutnya adalah memulai proses pelatihan model. Dalam tahap ini, kita menggunakan fitur callbacks untuk memantau performa model selama training. Jika model tidak menunjukkan peningkatan performa dalam beberapa epoch, maka proses pelatihan akan dihentikan secara otomatis (early stopping).

Parameter yang digunakan dalam proses training antara lain:

batch_size = 8 untuk menentukan jumlah sampel per iterasi,

epochs = 50 sebagai jumlah maksimum iterasi pelatihan,

shuffle = True agar data diacak setiap epoch, dan

verbose = 1 untuk menampilkan progres training secara detail.
"""

# Callback untuk menghentikan training jika performa model stagnan
early_stopping = tf.keras.callbacks.EarlyStopping(
    min_delta=0.0001,            # Perubahan minimal pada loss untuk dianggap sebagai peningkatan
    patience=7,                  # Jumlah epoch tanpa peningkatan sebelum berhenti
    restore_best_weights=True    # Mengembalikan bobot terbaik setelah training dihentikan
)

# Proses pelatihan model
history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=8,
    epochs=50,
    shuffle=True,
    validation_data=(x_val, y_val),
    verbose=1,
    callbacks=[early_stopping]
)

"""hasil train memperoleh nilai mean_absolute_error: 0.0337  dan root_mean_squared_error: 0.2233

## C. Pengujian Rekomendasi Movie CF

Pengguna sebelumnya telah memberikan rating pada sejumlah film yang pernah mereka tonton. Data rating ini kemudian kita manfaatkan untuk menghasilkan rekomendasi film yang relevan dan sesuai dengan preferensi pengguna.
"""

movies_df = movies_new
df = pd.read_csv('/content/movie_recs/ratings_small.csv')

# Mengambil sample user
user_id = df.userId.sample(1).iloc[0]
movie_watch_by_user = df[df.userId == user_id]

# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html
movie_not_watch = movies_df[~movies_df['id'].isin(movie_watch_by_user.movieId.values)]['id']
movie_not_watch = list(
    set(movie_not_watch)
    .intersection(set(movie_to_movie_encoded.keys()))
)

movie_not_watch = [[movie_to_movie_encoded.get(x)] for x in movie_not_watch]
user_encoder = user_to_user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movie_not_watch), movie_not_watch)
)

"""Kemudian, untuk memperoleh rekomendasi film, gunakan fungsi `model.predict()` dari library Keras dengan menerapkan kode berikut."""

# Prediksi rating untuk kombinasi user dan film yang belum ditonton
predicted_ratings = model.predict(user_movie_array).flatten()

# Ambil indeks 10 rating tertinggi secara terbalik (descending)
top_indices = predicted_ratings.argsort()[-10:][::-1]

# Mapping indeks ke movie ID asli menggunakan kamus decoding
recommended_movie_ids = [
    movie_encoded_to_movie.get(movie_not_watch[idx][0]) for idx in top_indices
]

print(f'Rekomendasi Film untuk Pengguna dengan ID: {user_id}\n')
print('----' * 20)
print('Film Favorit Pengguna Berdasarkan Rating Tertinggi:')
print('----' * 20)

# Ambil 5 film dengan rating tertinggi yang sudah ditonton pengguna
top_watched_movies = (
    movie_watch_by_user
    .sort_values(by='rating', ascending=False)
    .head(5)['movieId']
    .values
)

# Tampilkan informasi film favorit pengguna
fav_movies = movies_df[movies_df['id'].isin(top_watched_movies)]
for movie in fav_movies.itertuples():
    print(f"{movie.movie_name} : {movie.genres}")

print('\n' + '----' * 20)
print('10 Rekomendasi Film Terbaik:')
print('----' * 20)

# Tampilkan daftar film rekomendasi dengan nomor urut dan detail
recommended_movies = movies_df[movies_df['id'].isin(recommended_movie_ids)]
for i, movie in enumerate(recommended_movies.itertuples(), start=1):
    print('----' * 20)
    print(f'No         : {i}')
    print(f'Nama Film  : {movie.movie_name}')
    print(f'Genre      : {movie.genres}')
    print('----' * 20)

"""## D. Evaluation

### Metriks Visualisasi

Pada tahap ini, kita akan memvisualisasikan metrik evaluasi seperti Mean Absolute Error (MAE) dan Root Mean Squared Error (RMSE). Kedua metrik ini sangat penting untuk menilai seberapa akurat model dalam memprediksi nilai aktual. Dengan memplot grafik kedua metrik tersebut, kita dapat melihat perbandingan performa model serta mengamati tren perubahan kesalahan selama proses pelatihan, sehingga memudahkan analisis dan pengambilan keputusan terkait peningkatan model.

#### Plot Mean Absolute Error

melihat apa saja yang menjadi kunci untuk metriks evluasi
"""

print(history.history.keys())

plt.figure(figsize=(8,5))

plt.plot(history.history['MAE'], label='Train MAE', color='#2ca02c', linewidth=2)      # hijau mint
plt.plot(history.history['val_MAE'], label='Validation MAE', color='#d62728', linewidth=2)  # merah muda (lebih ke merah agak terang)

plt.title('Model Mean Absolute Error Over Epochs', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Mean Absolute Error (MAE)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""Berdasarkan grafik 'Model Mean Absolute Error Over Epochs', pada epoch terakhir (epoch ke-9), nilai MAE untuk data training (hijau) mencapai sekitar 0.038, sementara nilai MAE untuk data validasi (merah) berada di sekitar 0.173

#### Root Mean Squared Error Plot
"""

plt.figure(figsize=(8,5))

plt.plot(history.history['RMSE'], label='Train RMSE', color='#2ca02c', linewidth=2)       # hijau mint
plt.plot(history.history['val_RMSE'], label='Validation RMSE', color='#d62728', linewidth=2)  # merah muda

plt.title('Model Root Mean Squared Error Over Epochs', fontsize=14)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Root Mean Squared Error (RMSE)', fontsize=12)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""Berdasarkan grafik 'Model Root Mean Squared Error Over Epochs', pada epoch terakhir (epoch ke-9), nilai RMSE untuk data training (hijau) mencapai sekitar 0.048, sementara nilai RMSE untuk data validasi (merah) berada di sekitar 0.223."""